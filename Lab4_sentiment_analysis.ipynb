{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnxbJzbv_Jsr"
   },
   "source": [
    "# Lab 4 - Sentiment analysis from texts\n",
    "\n",
    "In this lab, you will learn:\n",
    "* How to clean texts\n",
    "* How to generate Document-Term matrix from texts\n",
    "* How to do sentiment analysis from texts\n",
    "\n",
    "This lab is written by Jisun AN (jisunan@smu.edu.sg) and Michelle KAN (michellekan@smu.edu.sg).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBe-9KkA_Jsw"
   },
   "source": [
    "# 1. Getting the data\n",
    "\n",
    "In this lab, we will use restaurant review data. \n",
    "\n",
    "This data is manually annotated by humans according to their aspect and sentiment. \n",
    "\n",
    "One review may have two or more aspects and thus two ore more sentiment. \n",
    "\n",
    "We note that we excluded those conflicting reviews.\n",
    "\n",
    "\"restaurant_reviews.tsv\" is tab-separated file which fields are \"sid \\t text \\t aspect \\t sentiment.\" \n",
    "\n",
    "sid is review id, text is a review, aspect is one of five lables (food, service, ambience, price), sentiment is one of three lables (positive, negative, neutral). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1612105924296,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "7Yig1Syu_Jsx"
   },
   "outputs": [],
   "source": [
    "# Import Pandas to analyze the data\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1612105984386,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "nEqga9U6_Jsx",
    "outputId": "ddc4725b-8724-4bce-ed95-33e52989544a"
   },
   "outputs": [],
   "source": [
    "# Read the file using Pandas 'read_table' function (either read_table, read_csv is fine)\n",
    "ori_df = pd.read_table(\"https://raw.githubusercontent.com/anjisun221/css_codes/main/restaurant_reviews.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(ori_df.shape)\n",
    "ori_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1612105994292,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "qCAV4Vqy_Jsy",
    "outputId": "fb812007-d300-4556-b5c8-055411cccbe6"
   },
   "outputs": [],
   "source": [
    "# to see entire text \n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "ori_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1612105998679,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ZuBAg7Cj_Jsy",
    "outputId": "da297cb1-83b9-42d2-d8f1-98a3d91082db"
   },
   "outputs": [],
   "source": [
    "ori_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1612106000075,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "haRWORA2_Jsy",
    "outputId": "1dfd05a8-dcaf-43f3-92a0-7adede39ef5a"
   },
   "outputs": [],
   "source": [
    "ori_df['aspect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDlS1tMF_Jsz"
   },
   "source": [
    "### Combine review by aspect + sentiment (e.g., all positive reviews about food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1612106006979,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "qsGKDAQt_Jsz",
    "outputId": "73c02705-fb73-4d97-c900-976939bb87c7"
   },
   "outputs": [],
   "source": [
    "list_sentiment = ['positive', 'negative', 'neutral']\n",
    "list_aspect = ['food', 'service', 'ambience', 'price']\n",
    "\n",
    "data_combined = {}\n",
    "\n",
    "for each_sent in list_sentiment:\n",
    "    for each_aspect in list_aspect:\n",
    "        \n",
    "        new_label = each_aspect+\"_\"+each_sent\n",
    "        print(new_label)\n",
    "        \n",
    "        tmp_df = ori_df.query(\"sentiment==@each_sent and aspect==@each_aspect\")\n",
    "        texts = \" \".join(tmp_df['text'].to_list())\n",
    "        data_combined[new_label] = [texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1278,
     "status": "ok",
     "timestamp": 1612106010925,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "JuFzGRt5_Jsz",
    "outputId": "5829ee1e-4ba3-4001-f40a-65662d9de6eb"
   },
   "outputs": [],
   "source": [
    "data_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S0QVEap_Jsz"
   },
   "source": [
    "Turn dictionary into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1612106015126,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "As2_tu9-_Jsz",
    "outputId": "13a642c3-9a23-4696-beaa-7f6908219514"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_combined, orient='index')\n",
    "df.columns = ['text']\n",
    "df = df.sort_index()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1612106018759,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "x44xtpEH_Js0",
    "outputId": "c5d04ef2-caa7-4a48-bf79-70a5c55d10d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the negative reviews for ambience\n",
    "df.text.loc['ambience_negative']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DcUkYyM_Js0"
   },
   "source": [
    "Let's save this dataframe to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79d-nz2a_Js0"
   },
   "source": [
    "# 2. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orQH1Z1P_Js0"
   },
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egoiXlSq_Js0"
   },
   "source": [
    "### Round 1. Let's make text lowercase, remove punctuations, remove words containing numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1612106024818,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "JSKxJUWu_Js1"
   },
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1612106026400,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "SF4n0-7Z_Js1",
    "outputId": "a5e94a51-eac3-4a6d-926e-f62de04fa6b5"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "df_clean = pd.DataFrame(df['text'].apply(round1))\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPvxSGDd_Js1"
   },
   "source": [
    "### Round 2. Let's remove stopwords. \n",
    "\n",
    "A stop word is a commonly used word (such as \"the\", \"a\", \"an\", \"in\"). For some analysis, like sentiment analysis, those stop words are often meaningless, and thus we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2915,
     "status": "ok",
     "timestamp": 1612106032521,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "0IG3CWjb_Js2",
    "outputId": "b612b801-6b29-4ef0-8950-b77e1c292d50"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1612106034312,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "8mqluk0w_Js2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "round2 = lambda x: ' '.join([word for word in x.split() if word not in (stop)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1612106036076,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "gPXZ2o6k_Js2",
    "outputId": "796e6b25-d30f-4338-b6bd-1803195a23f2"
   },
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame(df_clean['text'].apply(round2))\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4HjENg8_Js2"
   },
   "source": [
    "**NOTE:** This data cleaning aka text pre-processing step could go on for a while, but we are going to stop for now. After going through some analysis techniques, if you see that the results don't make sense or could be improved, you can come back and make more edits such as:\n",
    "* Mark 'outstanding' and 'outstand' as the same word (stemming / lemmatization)\n",
    "* Combine 'thank you' into one term (bi-grams)\n",
    "* And a lot more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPbFZzLi_Js2"
   },
   "source": [
    "## Organizing the data\n",
    "\n",
    "We will have clean, organized data in two standard text formats:\n",
    "\n",
    "1. **Corpus - **a collection of text\n",
    "2. **Document-Term Matrix - **word counts in matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gQsj1Ca_Js3"
   },
   "source": [
    "### Corpus\n",
    "\n",
    "We already created a corpus in an earlier step. The definition of a corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1612106042530,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "tIjzk5Yb_Js3",
    "outputId": "1617e8bc-9fd4-4d34-cc6a-178353a3d190"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1612106056063,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "mDGV8HOh_Js3",
    "outputId": "62abbbad-38b0-40f0-f5a3-622a0d995727",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's add the full label as well\n",
    "full_labels = ['ambience_negative', 'ambience_neutral', 'ambience_positive',\n",
    "       'food_negative', 'food_neutral', 'food_positive', 'price_negative',\n",
    "       'price_neutral', 'price_positive', 'service_negative',\n",
    "       'service_neutral', 'service_positive']\n",
    "\n",
    "df['category'] = full_labels\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1612106067534,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ZsRt-4Q0_Js3"
   },
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "df.to_pickle(\"corpus.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH0CjMNZ_Js3"
   },
   "source": [
    "### Document-Term Matrix\n",
    "\n",
    "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1612106105702,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "rX-V1RKW_Js4",
    "outputId": "3a7d1d6e-5b2a-4723-bc43-4227359b1dca"
   },
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english') #Iyou can remove stop words using CountVectorizer as well\n",
    "data_cv = cv.fit_transform(df_clean.text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = df.index\n",
    "data_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1612106109049,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "zk7qmIhF_Js4"
   },
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1612106111436,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "vJ2b7u-Q_Js4"
   },
   "outputs": [],
   "source": [
    "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "import pickle\n",
    "df_clean.to_pickle('data_clean.pkl')\n",
    "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbDt9BM2_Js4"
   },
   "source": [
    "## Additional Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bz3QE19_Js4"
   },
   "source": [
    "1. Can you add an additional regular expression to the clean_text_round2 function to further clean the text?\n",
    "2. Play around with CountVectorizer's parameters. What is ngram_range? What is min_df and max_df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAabM6_y_Js4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azMBzU-o_Js5"
   },
   "source": [
    "# 3. Expolatory Data Analysis\n",
    "\n",
    "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.\n",
    "\n",
    "We are going to look at the **Most common words** and **Amount of love/hate** for each category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSH1rQaG_Js5"
   },
   "source": [
    "## Most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1612106116036,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "IL8MZfWt_Js5",
    "outputId": "e4d102b9-638d-4d82-9719-40329bc32427"
   },
   "outputs": [],
   "source": [
    "# If you start from Section 3, please uncomment below code\n",
    "# import pandas as pd\n",
    "# data_dtm = pd.read_pickle('./dtm.pkl')\n",
    "\n",
    "data = data_dtm.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1612106119428,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "wodQnuj7_Js5",
    "outputId": "e80d8b29-d0a4-41bc-cb60-9657c737c104"
   },
   "outputs": [],
   "source": [
    "# Find the top 30 words said by each category\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1612106132722,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "h-dMilJt_Js5",
    "outputId": "75d265e7-231b-48ae-91e0-b38c6e928415",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the top 15 words said by each category\n",
    "for category, top_words in top_dict.items():\n",
    "    print(category)\n",
    "    print(', '.join([word for word, count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8BoMXQr_Js6"
   },
   "source": [
    "**NOTE:** At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDge8zuF_Js6"
   },
   "outputs": [],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each category\n",
    "words = []\n",
    "for category in data.columns:\n",
    "    top = [word for (word, count) in top_dict[category]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1612106184173,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "PYNz97Te_Js6",
    "outputId": "149605c4-8b02-47c7-da34-c8cfc661d631"
   },
   "outputs": [],
   "source": [
    "# Let's aggregate this list and identify the most common words along with how many categories they occur in\n",
    "Counter(words).most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1612106198400,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "ipjL7OIW_Js6",
    "outputId": "594eb3f1-1fcc-4c02-900d-6c6e3c1c8dfc"
   },
   "outputs": [],
   "source": [
    "# If more than half of the categories (6) have it as a top word, exclude it from the list \n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1612106203541,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "FONfTCS8_Js6"
   },
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "df_clean = pd.read_pickle('./data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(df_clean.text)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = df_clean.index\n",
    "\n",
    "# # Pickle it for later use\n",
    "# If you start from Section 3, please uncomment below code\n",
    "# import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1612106210655,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "tu3PC97e_Js7"
   },
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 6905,
     "status": "ok",
     "timestamp": 1612106222185,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "SXPXVyRu_Js7",
    "outputId": "d037f010-c833-4388-d865-35b25747c869"
   },
   "outputs": [],
   "source": [
    "# Reset the output dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "full_labels = ['ambience_negative', 'ambience_neutral', 'ambience_positive',\n",
    "       'food_negative', 'food_neutral', 'food_positive', 'price_negative',\n",
    "       'price_neutral', 'price_positive', 'service_negative',\n",
    "       'service_neutral', 'service_positive']\n",
    "\n",
    "# Create subplots for each category\n",
    "for index, category in enumerate(data.columns):\n",
    "    wc.generate(df_clean.text[category])\n",
    "    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(full_labels[index])\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7BtvXWT_Js7"
   },
   "source": [
    "Findings \n",
    "\n",
    "* Reviews of different aspects seems to have different set of frequent words. E.g., ambience : table, atmosphere, decor, etc whiel price : price, worth, cheap, etc.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSdQYoda_Js8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Tk34a_F_Js8"
   },
   "source": [
    "## Amount of love/hate words in positive/negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1612106271352,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "1_PPsyKI_Js8",
    "outputId": "5e648828-ef8f-4690-f7fb-ad7b2ca93235"
   },
   "outputs": [],
   "source": [
    "# Let's isolate just these words\n",
    "data_lovehate_words = data.transpose()[['love', 'best', 'dont', 'worst', 'didnt']]\n",
    "data_lovehate = pd.concat([data_lovehate_words.love + data_lovehate_words.best, data_lovehate_words.dont + data_lovehate_words.worst + data_lovehate_words.didnt], axis=1)\n",
    "data_lovehate.columns = ['love', 'hate']\n",
    "data_lovehate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "executionInfo": {
     "elapsed": 1255,
     "status": "ok",
     "timestamp": 1612106274325,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "9igvKHXy_Js9",
    "outputId": "8d53400a-b084-427e-8c55-03e3cd9dcd4d"
   },
   "outputs": [],
   "source": [
    "# Let's create a scatter plot of our findings\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, category in enumerate(data_lovehate.index):\n",
    "    x = data_lovehate.love.loc[category]\n",
    "    y = data_lovehate.hate.loc[category]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+1.5, y+0.5, full_labels[i], fontsize=10)\n",
    "    plt.xlim(-5, 85) \n",
    "\n",
    "plt.title('Number of Love/Hate Words Used in Reviews', fontsize=20)\n",
    "plt.xlabel('Number of love/best', fontsize=15)\n",
    "plt.ylabel('Number of dont/didnt/worst', fontsize=15)\n",
    "\n",
    "xpoints = ypoints = plt.xlim()\n",
    "plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=3, scalex=False, scaley=False)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_gXhck-_Js9"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huKrLjIK_Js9"
   },
   "source": [
    "What other word counts do you think would be interesting to compare instead of the love/hate words? Create a scatter plot comparing them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mHAbHDc_Js-"
   },
   "outputs": [],
   "source": [
    "# Write your code here \n",
    "# If you get an error that you don't have 'data' defined, please uncomment below\n",
    "# import pandas as pd\n",
    "# data_dtm = pd.read_pickle('./dtm.pkl')\n",
    "# data = data_dtm.transpose()\n",
    "\n",
    "\n",
    "# Let's isolate some words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFZzPHZI_Js-"
   },
   "outputs": [],
   "source": [
    "# Let's create a scatter plot of your findings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCIdpkE6_Js-"
   },
   "source": [
    "# 4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CMw56mj_Js-"
   },
   "source": [
    "We will examine whether sentiment analysis method is useful to distinguish positive/neutral/negative reviews. \n",
    "\n",
    "In this lab, we will use **TextBlob** for sentiment analysis.\n",
    "\n",
    "1. **TextBlob Module:** Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence. The TextBlob module allows us to take advantage of these labels.\n",
    "2. **Sentiment Labels:** Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we're going to ignore them for now). A corpus' sentiment is the average of these.\n",
    "   * **Polarity**: How positive or negative a word is. -1 is very negative. +1 is very positive.\n",
    "   * **Subjectivity**: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion.\n",
    "\n",
    "For more info on how TextBlob coded up its [sentiment function](https://planspace.org/20150607-textblob_sentiment/).\n",
    "\n",
    "Let's take a look at the sentiment of the various categories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1612106300312,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "zlTDCtl0_Js-",
    "outputId": "dfcd2b42-23ac-4cd3-e104-5f5ccba518d8"
   },
   "outputs": [],
   "source": [
    "# If you start from Section 4, please uncomment below code\n",
    "# import pandas as pd\n",
    "\n",
    "# We'll start by reading in the corpus, which preserves word order\n",
    "data = pd.read_pickle('./corpus.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROrerxbw_Js-"
   },
   "outputs": [],
   "source": [
    "# install textblob\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "executionInfo": {
     "elapsed": 1503,
     "status": "ok",
     "timestamp": 1612106312559,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "7S0-z7jg_Js_",
    "outputId": "acf79474-f3b0-4f90-98e8-b3d7670a9cfc"
   },
   "outputs": [],
   "source": [
    "# Create quick lambda functions to find the polarity and subjectivity of each category\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "data['polarity'] = data['text'].apply(pol)\n",
    "data['subjectivity'] = data['text'].apply(sub)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 1400,
     "status": "ok",
     "timestamp": 1612106326223,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "J0hCzEoq_Js_",
    "outputId": "e1d39d90-c7fc-4b5b-c768-1b017898afeb"
   },
   "outputs": [],
   "source": [
    "# Let's plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for index, category in enumerate(data.index):\n",
    "    x = data.polarity.loc[category]\n",
    "    y = data.subjectivity.loc[category]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+.001, y+.001, data['category'][index], fontsize=10)\n",
    "#     plt.xlim(-.01, .12) \n",
    "    plt.xlim(-.1, .55) \n",
    "    \n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
    "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_u8jKW_d_Js_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pG8eNJi_Js_"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let's compare the sentiments of the reviews computed by Textblob and Vader\n",
    "\n",
    "You will need to apply Vader to analyze sentiment of reviews. \n",
    "\n",
    "1. Define the function that return compound score given a sentence\n",
    "2. Apply function 1 to compute vader score, the column name would be 'vader_sent' \n",
    "3. Draw a scatter plot to compare Vader sentiment score (x-axis) with TextBlob polarity score (y-axis)\n",
    "4. What's your conclusion?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4255,
     "status": "ok",
     "timestamp": 1612106339713,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "O1Adx8Cu_Js_",
    "outputId": "14d03a3d-8b91-4506-fa1a-a730d169bc16"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1838,
     "status": "ok",
     "timestamp": 1612106339714,
     "user": {
      "displayName": "Jisun An",
      "photoUrl": "",
      "userId": "09226046566822916181"
     },
     "user_tz": -480
    },
    "id": "NGD8Vg3w_Js_"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1ZP8cj9_Js_"
   },
   "outputs": [],
   "source": [
    "# 1. Write a python function that returns VADER's 'compound score' of a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4xrKg0V_JtA"
   },
   "outputs": [],
   "source": [
    "# 2. Apply vader_compound_score on data. New column name would be 'vader_sent'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A7CR4l9_JtA"
   },
   "outputs": [],
   "source": [
    "# 3. Let's draw scatter plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86IxwLUC_JtA"
   },
   "source": [
    "\n",
    "4. Conclusion\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1M3oOXD_JtA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOS6yCkd_JtA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_sentiment_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
